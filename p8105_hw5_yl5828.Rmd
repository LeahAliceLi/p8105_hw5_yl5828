---
title: "p8105_hw5_yl5828"
author: "Leah Li"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(rvest)
library(broom)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```



### Problem 1

#### Create the required function to check if there are any duplicate birthdays.

```{r create function for checking duplicate}
birthday_sim <- function(n_people) {
  birthdays <- sample(1:365, size = n_people, replace = TRUE)
  any(duplicated(birthdays))
}
```

#### Repeat the function 10000 times and compute and probablity that at least two people in the group will share a birthday.

```{r repeat and find average probablity}
set.seed(1)

sim_results <- 
  tibble(group_size = 2:50) %>% 
  mutate(
    sim_outcomes = map(group_size, ~ replicate(10000, birthday_sim(.x))),
    prob_shared  = map_dbl(sim_outcomes, mean)
  )
```

#### Creae a plot to show the results.
```{r plot graph}
ggplot(sim_results, aes(x = group_size, y = prob_shared)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    x = "Group size (n)",
    y = "Probability of ≥ 1 shared birthday",
    title = "Q1: Simulated Probability of Shared Birthdays"
  ) +
  theme_minimal()
```

#### Comment on the plot:
The curve shows an exponential-like growth, rapidly increasing from near-zero probability with small groups to near-certainty with larger groups.
The simulation shows that with just 23 people, there's approximately a 50% chance that two people share a birthday. With 30 people, the probability rises to about 70%. With 40 people, the probability exceeds 90%. With 50 people, the probability is nearly 97%.


### Problem 2

#### Define parameters
```{r define parameters}
n     <- 30
sigma <- 5
mu_vals <- 0:6  
n_sims <- 5000
```

#### Create a function to generate one dataset and run one-sample t-test.
```{r create one sample t-test function}
sim_one <- function(mu, n = 30, sigma = 5) {
  x <- rnorm(n, mean = mu, sd = sigma)
  
  t.test(x, mu = 0) |>
    tidy() |>
    transmute(
      mu_hat  = estimate,
      p_value = p.value
    )
}
```

#### Run the simulation of 5000 datasets for each mu value
```{r run the function for 5000 datasets}
sim_results <- map_dfr(mu_vals, function(mu) {
  tibble(
    mu_true = mu,
    sim_id  = 1:n_sims
  ) |>
    mutate(
      res = map(sim_id, ~ sim_one(mu))
    ) |>
    unnest(cols = res)
})

head(sim_results)
```

#### Plot1: Power vs. effect size

```{r plot1 Power vs. effect size}
power_df <- sim_results %>%
  group_by(mu_true) %>%
  summarize(
    power = mean(p_value < 0.05),
    .groups = "drop"
  )

ggplot(power_df, aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True mean μ",
    y = "Power (Pr reject H0: μ = 0)",
    title = "Power of one-sample t-test (n = 30, σ = 5)"
  ) +
  theme_minimal()

```

#### Comment:
Power is about 0.05 when μ = 0 (just the type I error rate). As the true mean moves farther from 0 (larger effect size), power increases rapidly and approaches 1 by μ around 5–6. So larger effect sizes leads to higher power.


#### Plot2: Average Estimates vs. True mu

```{r plot2 Average Estimates vs. True mu}
mu_summary <- sim_results %>%
  group_by(mu_true) %>%
  summarize(
    mean_mu_hat_all    = mean(mu_hat),
    mean_mu_hat_reject = mean(mu_hat[p_value < 0.05]),
    .groups = "drop"
  )

mu_summary
```

```{r}
ggplot(mu_summary, aes(x = mu_true)) +
  geom_line(aes(y = mean_mu_hat_all), linewidth = 1) +
  geom_point(aes(y = mean_mu_hat_all)) +
  geom_line(aes(y = mean_mu_hat_reject), linetype = "dashed") +
  geom_point(aes(y = mean_mu_hat_reject), shape = 21, fill = "white") +
  labs(
    x = "True mean μ",
    y = "Average estimate of μ̂",
    title = "Average μ̂ overall vs. only among significant tests",
    subtitle = "Solid = all samples, Dashed = only where H0 was rejected"
  ) +
  theme_minimal()

```

#### Comment:
The solid line (all samples): Falls perfectly on the 45° diagonal line, showing that μ̂ is an unbiased estimator of μ across all samples
The dotted line (only rejected nulls): Shows overestimation when power is low (small effect sizes), then converges to the true value as power increases



### Problem 3


#### Read in dataset
```{r read in data}
url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides <- read_csv(url)
```

#### Create variable and sum totals and counts
```{r create required variable}
homicides_city <- homicides %>%
  mutate(
    city_state = str_c(city, ", ", state),
    is_unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )

city_summary <- homicides_city %>%
  group_by(city_state) %>%
  summarize(
    total = n(),
    unsolved = sum(is_unsolved),
    .groups = "drop"
  )

city_summary
```

#### Estimate and CI for Baltimore
```{r baltimore results}
baltimore <- city_summary %>%
  filter(city_state == "Baltimore, MD")

baltimore

baltimore_prop <- prop.test(
  x = baltimore$unsolved,
  n = baltimore$total
)

baltimore_tidy <- tidy(baltimore_prop)
baltimore_tidy

# pull out estimate and CI
baltimore_estimate <- baltimore_tidy %>% pull(estimate)
baltimore_ci_low   <- baltimore_tidy %>% pull(conf.low)
baltimore_ci_high  <- baltimore_tidy %>% pull(conf.high)

baltimore_estimate
baltimore_ci_low
baltimore_ci_high
```

#### Run the same for every city
```{r every city result}
city_results <- city_summary %>%
  mutate(
    prop_test = map2(unsolved, total, ~ prop.test(.x, .y)),
    prop_tidy = map(prop_test, tidy)
  ) %>%
  unnest(prop_tidy)

city_results
```

#### Plot the estimates with CIs for each city
```{r plot graphs}
city_results %>%
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    x = "",
    y = "Proportion of homicides unsolved",
    title = "Estimated proportion of unsolved homicides by city",
    subtitle = "95% confidence intervals from prop.test"
  ) +
  theme_minimal()
```






